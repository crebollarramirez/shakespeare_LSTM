model: 'LSTM_no_teacher_forcing'

embed_size: 25

hidden_size: 50

num_layers: 2

epochs: 10

patience: 3

seq_len: 8

learning_rate: 0.001

log_interval: 5

batch_size: 64

save_path: "models"
